                                     DAY 2- LAB PROGRAMS

1.Covariance and correlation 
Children of three ages are asked to indicate their preference for three photographs of adults. Do the data suggest that there is a significant relationship between age and photograph preference? What is wrong with this study?

                                                                        Photograph:
                         Age of child               A                      B                      C
         				   5-6 years:      18                     22                     20

                                    7-8 years:         2                    28                     40

                                    9-10 years:     20                     10                     40

1.	Use cov() to calculate the sample covariance between B  and  C.
2.	Use another call to cov() to calculate the sample covariance matrix for the preferences.
3.	Use cor() to calculate the sample correlation between B and C.
4.	Use another call to cor() to calculate the sample correlation matrix for the preferences.
CODING-
age_of_child<-c("5-6 years","7-8 years","9-10 years")
A<-c(18,2,20)
B<-c(22,28,10)
C<-c(20,40,40)
df<-data.frame(age_of_child,A,B,C)
df
#1a
covar<-cor(B,C)
covar
#1b
cov(df[c(2,3,4)])
#1c
cor(B,C)
#1d
cor(df[c(2,3,4)])

OUTPUT-
#1a
[1] -0.1889822
#1b
> cov(df[c(2,3,4)])
          A   B         C
A  97.33333 -74 -46.66667
B -74.00000  84 -20.00000
C -46.66667 -20 133.33333
#1c
[1] -0.1889822
#1d
> cor(df[c(2,3,4)])
           A          B          C
A  1.0000000 -0.8183918 -0.4096440
B -0.8183918  1.0000000 -0.1889822
C -0.4096440 -0.1889822  1.0000000

2.Imagine that you have selected data from the All Electronics data warehouse for analysis. The data set will be huge! The following data are a list of All Electronics prices for commonly sold items (rounded to the nearest dollar). The numbers have been sorted: 1, 1, 5, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 14, 15, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18, ,20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 25, 25, 25, 25, 25, 28, 28, 30)
30, 30. 
(i) Partition the dataset using an equal-frequency partitioning method with bin equal to 3 (ii) apply data smoothing using bin means and bin boundary.
(iii) Plot Histogram for the above frequency division 

CODING-
#2
data<-c(1, 1, 5, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 14, 15, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18,20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 25, 25, 25, 25, 25, 28, 28, 30)
bin=3
#2a-equal frequency
freq<-length(data)/bin
print(freq)
#2b-bins mean
range=ceiling(freq)
bin1=c()
bin2=c()
bin3=c()
for(i in data[1:range]){
  bin1=append(bin1,i)
}
range1=range+1
range2=range*2
for(j in data[range1:range2])
{
  bin2=append(bin2,j)
}
range3=range2+1
range4=range*3
for(k in data[range3:range4])
{
  bin3=append(bin3,k)
}
mean(bin1)
mean(bin2)
new_bin3<-bin3[! bin3%in% c(NA)]
mean(new_bin3)
#2c
hist(bin1)
hist(bin2)
hist(bin3)
OUTPUT-
#2a
[1] 15.33333

#2b
> mean(bin1)
[1] 7.6875
> mean(bin2)
[1] 17.125
> mean(new_bin3)
[1] 23.92857
#2c
  


 

3.Two Maths teachers are comparing how their Year 9 classes performed in the end of year exams. Their results are as follows:
Class A: 76, 35, 47, 64, 95, 66, 89, 36, 8476,35,47,64,95,66,89,36,84 
Class B: 51, 56, 84, 60, 59, 70, 63, 66, 5051,56,84,60,59,70,63,66,50 
(i) Find which class had scored higher mean, median and range.
(ii) Plot above in boxplot and give the inferences 

CODING-
#3a
class_A<-c(76, 35, 47, 64, 95, 66, 89, 36, 8476,35,47,64,95,66,89,36,84) 
class_B<-c(51, 56, 84, 60, 59, 70, 63, 66, 5051,56,84,60,59,70,63,66,50)
mean_a<-mean(class_A)
mean_a
mean_b<-mean(class_B)
mean_b
median_a<-median(class_A)
median_a
median_b<-median(class_B)
median_b
range_a<-max(class_A)-min(class_A)
range_a
range_b<-max(class_B)-min(class_B)
range_b
print(range_a)
print(range_b)
#3b-boxplot
boxplot(class_A)
boxplot(class_B)
#3b-inferences
if (mean_a > mean_b) {
  cat("Class A had a higher mean score.\n")
} else if (mean_a < mean_b) {
  cat("Class B had a higher mean score.\n")
} else {
  cat("Both classes had the same mean score.\n")
}

if (median_a > median_b) {
  cat("Class A had a higher median score.\n")
} else if (median_a < median_b) {
  cat("Class B had a higher median score.\n")
} else {
  cat("Both classes had the same median score.\n")
}

if (range_a > range_b) {
  cat("Class A had a higher range of scores.\n")
} else if (range_a < range_b) {
  cat("Class B had a higher range of scores.\n")
} else {
  cat("Both classes had the same range of scores.\n")
}

OUTPUT-
> mean_a
[1] 558.8235
> mean_b
[1] 356.9412
> median_a
[1] 66
> median_b
[1] 63
> print(range_a)
[1] 8441
> print(range_b)
[1] 5001
#3b
Class a                                                                                            class b
  

#3C
Class A had a higher mean of scores
Class A had a higher median of scores
Class A had a higher range of scores
4) Let us consider one example to make the calculation method clear. Assume that the minimum and maximum values for the feature F are $50,000 and $100,000 correspondingly. It needs to range F from 0 to 1. In accordance with min-max normalization, v = $80,
b) Use the two methods below to normalize the following group of data: 200, 300, 400, 600, 1000
 (a) min-max normalization by setting min = 0 and max = 1
 (b) z-score normalization
#4
v<-80
min<-50000
max<-100000
result1=v-min
result2=max-min
result3=result1/result2
print(result3)

#min max normalization
data <- c(200, 300, 400, 600, 1000)
min<-min(data)
max<-max(data)
for (i in data)
{
  result1=i-min
  result2=max-min
  result3=result1/result2
  print(result3)
}  

#z score
data <- c(200, 300, 400, 600, 1000)
mean1<-mean(data)
deviation<-sd(data)
for (i in data)
{
  result1=i-mean1
  result2=result1/deviation
  print(result2)
}
OUTPUT-
[1] -0.9984
#min max normalization
[1] 0
[1] 0.125
[1] 0.25
[1] 0.5
[1] 1
#z score normalization
[1] -0.9486833
[1] -0.6324555
[1] -0.3162278
[1] 0.3162278
[1] 1.581139




5.Make a histogram for the “AirPassengers “dataset, start at 100 on the x-axis, and from values 200 to 700, make the bins 150 wide
CODING-
data("AirPassengers")
hist(AirPassengers, breaks = seq(100, 700, by = 150),  main=" Histogram for Airpassengers", xlab = "Passenger count", ylab = "Frequency")

OUTPUT-
 

6.Obtain Multiple Lines in Line Chart using a single Plot Function in R.Use attributes“mpg”and“qsec”of   the dataset “mtcars”

CODING-
plot(mtcars$mpg,type = "l",col="blue")
lines(mtcars$qsec,type="l",col="red")

OUTPUT-
 

7.Download the Dataset "water" From R dataset Link.Find out whether there is a linear relation between attributes"mortality" and"hardness" by plot function.Fit the Data into the Linear Regression model.Predict the mortality for the hardness=88.

CODING-
path <- "/Users/kadiv/OneDrive/Desktop/water_potability.csv"
content1 <- read.csv(path)
# contents of the csv file
print(content1)
#linear regression
linear_reg<-lm(ph~Hardness,data=content1)
new_var<-data.frame(Hardness = 88)
#predicted value with newdata
predict(linear_reg,newdata=new_var)

output-
1 
6.650547


8.Create a Boxplot graph for the relation between "mpg"(miles per galloon) and "cyl"(number of Cylinders) for the dataset "mtcars" available in R Environment.
CODING-
mtcars
boxplot(mtcars$mpg~mtcars$cyl,col='orange')
OUTPUT-
 


   9. Assume the Tennis coach wants to determine if any of his team players are scoring   
      outliers. To visualize the distribution of points scored by his players, then how can he    
      decide to develop the box plot? Give suitable example using Boxplot visualization   
      technique.

CODING-
players<-c("player1","player2","player3","player4","player5","player6","player7","player8","player9","player10")
points<-c(8,9,6,7,7,1,2,6,8,9)
data.frame(players,points)
boxplot(points)
#outliers are points-(1,2)
OUTPUT-
 
10. Implement using R language in which age group of people are affected byblood pressure based on the diabetes dataset show it using scatterplot and bar chart (that is BloodPressure vs Age using dataset “diabetes.csv”)

CODING-
#enter your copy path to diabetes.csv
path <- "/Users/kadiv/OneDrive/Desktop/Tools DWDM/diabetes.csv"
content <- read.csv(path)
# contents of the csv file
print(content)
plot(content$BloodPressure,content$Age,col='red',xlab='BloodPressue',ylab='Age')
barplot(content$BloodPressure,content$Age,xlab='BloodPressue',ylab='Age')

OUTPUT-
  

                                                          DWDM R PROGRAMMING-PRACTICALS
1.List of Programs:
1The intervals and corresponding frequencies are as follows. age frequency
1-5. 200 
5-15 450 
15-20 300 
20-50 1500 
50-80 700 
80-110 44
Compute an approximate median value for the data

CODING-
class_interval<-c("1-5","5-15","15-20","20-50","50-80","80-110")
data<-c(200,450,300,1500,700,44)
data.frame(class_interval,data)
median(data)

OUTPUT-
> data.frame(class_interval,data)
  class_interval data
1            1-5  200
2           5-15  450
3          15-20  300
4          20-50 1500
5          50-80  700
6         80-110   44
> median(data)
[1] 375

2. Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.
(a) What is the mean of the data? What is the median?
(b) What is the mode of the data? Comment on the data’s modality (i.e., bimodal, trimodal, etc.).
(c) What is the midrange of the data?
(d) Can you find (roughly) the first quartile (Q1) and the third quartile (Q3) of the data?
Coding:
#2a
 x<-c(13,15,16,16,19,20,20,21,22,22,25,25,25,25,30,33,33,35,35,35,35,36,40,45,46,52,70)
#mean
mean(x)
#median
median(x)
output:
mean(x)
[1] 29.96296
> #median
> median(x)
[1] 25
CODING FOR 2b-
#2b
 #mode
MultipleModes(age_values)
output:
25 35

CODING FOR 2c-
#midrange
c) age_values <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
median(age_values)
OUTPUT-
25
CODING FOR 2d-
d) #quartile
age_values <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
quantile(age_values)
output: 0%  25%  50%  75% 100% 
13.0 20.5 25.0 35.0 70.0


3.Data Preprocessing :Reduction and Transformation
   Use the two methods below to normalize the following group of data: 200, 300, 400, 600, 1000 (a) min-max normalization by setting min = 0 and max = 1 (b) z-score normalization
Coding:
#3a
data <- c(200, 300, 400, 600, 1000)
min<-min(data)
max<-max(data)
for (i in data)
{
  result1=i-min
  result2=max-min
  result3=result1/result2
  print(result3)
}  
OUTPUT:
[1] 0
[1] 0.125
[1] 0.25
[1] 0.5
[1] 1

#3b
 data <- c(200, 300, 400, 600, 1000)
mean1<-mean(data)
deviation<-sd(data)
for (i in data)
{
  result1=i-mean1
  result2=result1/deviation
  print(result2)
}
OUTPUT:
[1] -0.9486833
[1] -0.6324555
[1] -0.3162278
[1] 0.3162278
[1] 1.581139
4.Data:11,13,13,15,15,16,19,20,20,20,21,21,22,23,24,30,40,45,45,45,71,
72,73,75
 
      a) Smoothing by bin mean
b) Smoothing by bin median
c) Smoothing by bin boundaries
CODING-
#binning
data <- c(11, 13, 13, 15, 15, 16, 19, 20, 20, 20, 21, 21, 22, 23, 24, 30, 40, 45, 45, 45, 71, 72, 73, 75)
range=6
bin1=c()
bin2=c()
bin3=c()
bin4=c()
for(i in data[1:range]){
  bin1=append(bin1,i)
}
range1=range+1
range2=range*2
for(j in data[range1:range2])
{
  bin2=append(bin2,j)
}
range3=range2+1
range4=range*3
for(k in data[range3:range4])
{
  bin3=append(bin3,k)
}
range5=range4+1
range6=range*4
for(l in data[range5:range6]){
  bin4=append(bin4,l)
}
#4a
mean(bin1)
mean(bin2)
mean(bin3)
mean(bin4)
#4b
median(bin1)
median(bin2)
median(bin3)
median(bin4)
OUTPUT:
#4a
> mean(bin1)
[1] 13.83333
> mean(bin2)
[1] 20.16667
> mean(bin3)
[1] 30.66667
> mean(bin4)
[1] 63.5
>
 #4b
> median(bin1)
[1] 14
> median(bin2)
[1] 20
> median(bin3)
[1] 27
> median(bin4)
[1] 71.5
5) 5. Suppose that a hospital tested the age and body fat data for 18 randomly selected adults with the following results: 



 
CODING-
age <- c(23,23,27,27,39,41,47,49,50,52,54,54,56,57,58,58,60,61)
body_fat_percent <- c(9.5,26.5,7.8,17.8,31.4,25.9,27.4,27.2,31.2,34.6,42.5,28.8,33.4,30.2,34.1,32.9,41.2,35.7)
#5.a
mean(age)
mean(body_fat_percent)
median(age)
median(body_fat_percent)
sd(age)
sd(body_fat_percent)
#5.b
#create dataframe
df<-data.frame(age,body_fat_percent)
#box plot
boxplot(df)
#scatter plot
plot(df)
#qq plot
qqnorm(age)
qqline(age)
qqnorm(body_fat_percent)
qqline(body_fat_percent)
OUTPUT-
#5a
> mean(age)
 [1] 46.44444 
> mean(body_fat_percent)
 [1] 28.78333
 > median(age) 
[1] 51
 > median(body_fat_percent)
 [1] 30.7 
> sd(age)
 [1] 13.21862 
> sd(body_fat_percent)
 [1] 9.254395
#5.b
BOXPLOT-
 
SCATTER PLOT-
#5c

QQ  

QQ PLOT FOR AGE-
 

QQPLOT FOR BODY FAT PERCENT-
 

6.Suppose that a hospital tested the age and body fat data for 18 randomly selected adults with the following results: 
(i) Use min-max normalization to transform the value 35 for age onto the range [0.0, 1.0].
(ii) Use z-score normalization to transform the value 35 for age, where the standard deviation of age is 12.94 years.
(iii) Use normalization by decimal scaling to transform the value 35 for age. Perform the above functions using R – tool 

CODING-
age <- c(23,23,27,27,39,41,47,49,50,52,54,54,56,57,58,58,60,61)
new_age<-c()
for(i in age){
  if(i<=35){
    new_age=append(new_age,i)
  }
}
print(new_age)
#6a
#min max normalization
min<-min(new_age)
max<-max(new_age)
for (i in new_age)
{
  result1=i-min
  result2=max-min
  result3=result1/result2
  print(result3)
}  
#6b
#z score normalization
mean1<-mean(new_age)
for (i in new_age)
{
  result1=i-mean1
  result2=result1/12.94
  print(result2)
}
#6c
#decimal scaling
n=200
j=nchar(y)
scaling=n/10^j
print(scaling)

OUTPUT-

6.a MIN MAX NORMALIZATION
[1] 0
[1] 0
[1] 1
[1] 1
6.b Z SCORE NORMALIZATION
[1] -0.8660254
[1] -0.8660254
[1] 0.8660254
[1] 0.8660254
6.c DECIMAL SCALING
[1] 0.2

7.The following values are the number of pencils available in the different boxes. Create a vector and find out the mean, median and mode values of set of pencils in the given data. 
Box1 Box2 Box3 Box4 Box5 Box6 Box7 Box8 Box9 Box 10
9          25      23     12      11      6      7        8        9             10 

CODING-
box_no=c("box1","box2","box3","box4","box5","box6","box7","box8","box9","box10")
pencil=c(9,25,23,12,11,6,7,8,9,10)
df<-data.frame(box_no,pencil)
#dataframe
print(df)
#mean
mean(pencil)
#median
median(pencil)
#mode
mode=names(which.max(table(pencil)))
print(mode)

OUTPUT-
> data.frame(box_NO,pencil)
   box_NO pencil
1    box1      9
2    box2     25
3    box3     23
4    box4     12
5    box5     11
6    box6      6
7    box7      7
8    box8      8
9    box9      9
10  box10     10
> mean(pencil)
[1] 12
> median(pencil)
[1] 9.5
> print(mode)
[1] "9"
8. the following table would be plotted as (x,y) points, with the first column being the x values as number of mobile phones sold and the second column being the y values as money. To use the scatter plot for how many mobile phones sold. 
x :4 1 5 7 10 2 50 25 90 36 
y :12 5 13 19 31 7 153 72 275 110 

CODING-
x<-c(4, 1, 5, 7, 10, 2, 50, 25, 90, 36) 
y<-c(12,5, 13, 19, 31, 7, 153, 72, 275, 110) 
plot(x,y,xlab='MOBILE PHONES SOLD',ylab='MONEY')

OUTPUT-
 

 9. Implement of the R script using marks scored by a student in his model exam has been sorted as follows: 55, 60, 71, 63, 55, 65, 50, 55,58,59,61,63,65,67,71,72,75. Partition them into three bins by each of the following methods. Plot the data points using histogram. 
(a) equal-frequency (equi-depth) partitioning (b) equal-width partitioning 
CODING-
marks<-c(55, 60, 71, 63, 55, 65, 50, 55,58,59,61,63,65,67,71,72,75)
binning1=c()
binning2=c()
binning3=c()
range=6
#binning partition
for(a in marks[1:range]){
  binning1=append(binning1,a)
}
range1=range+1
range2=range*2
for(b in marks[range1:range2])
{
  binning2=append(binning2,b)
}
range3=range2+1
range4=range*3
for(c in marks[range3:range4])
{
  binning3=append(binning3,c)
}
print(binning1)
print(binning2)
print(binning3)
#histogram
hist(binning1)
hist(binning2)
hist(binning3)
#9a
#equal-frequency
freq=length(marks)/range
print(freq)
#9b
#equal-width
min<-min(marks)
max<-max(marks)
result<-max-min
width<-result/range
cat("width is",width)
bin1=width+min
print(bin1)
bin2=2*width+min
print(bin2)
bin3=3*width+min
print(bin3)
OUTPUT-


> print(binning1)
[1] 55 60 71 63 55 65
> print(binning2)
[1] 50 55 58 59 61 63
> print(binning3)
[1] 65 67 71 72 75 NA

HISTOGRAM-
 
  
#9a
#equal frequency
> print(freq)
[1] 2.833333
#9b
#equal width
width is 4.166667> bin1=width+min
> print(bin1)
[1] 54.16667
> bin2=2*width+min
> print(bin2)
[1] 58.33333
> bin3=3*width+min
> print(bin3)
[1] 62.5
10. Suppose that the speed car is mentioned in different driving style. 
Regular 78.3 81.8 82 74.2 83.4 84.5 82.9 77.5 80.9 70.6 Speed 
Calculate the Inter quantile and standard deviation of the given data. 
CODING-
speed<-c(78.3 ,81.8 ,82 ,74.2 ,83.4 ,84.5 ,82.9 ,77.5 ,80.9 ,70.6 )
#interquartile
IQR(speed)
#standard deviation
sd(speed)
OUTPUT-
> IQR(speed)
[1] 4.975
> sd(speed)
[1] 4.445835
11.Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.
Can you find (roughly) the first quartile (Q1) and the third quartile (Q3) of the data?
CODING-
marks<-c(13,15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
quantile(marks)
OUTPUT-
> quantile(marks)
  0%  25%  50%  75% 100% 
13.0 20.5 25.0 35.0 70.0


# Load the necessary libraries
library(ggplot2)
library(readr)

# Read the diabetes dataset from the URL
diabetes_data <- read_csv("https://raw.githubusercontent.com/bhargavsai19/datasets/main/diabetes.csv")

# Display the structure of the dataset
str(diabetes_data)

# Create a scatterplot
scatter_plot <- ggplot(data = diabetes_data, aes(x = Age, y = BloodPressure)) +
  geom_point() +
  labs(title = "Scatterplot of BloodPressure vs Age",
       x = "Age", y = "BloodPressure")
print(scatter_plot)

# Create age groups
age_group_data <- diabetes_data %>%
  mutate(AgeGroup = cut(Age, breaks = c(20, 30, 40, 50, 60, 70, Inf),
                        labels = c("20-29", "30-39", "40-49", "50-59", "60-69", "70+")))

# Calculate average blood pressure by age group
avg_bp_by_age_group <- age_group_data %>%
  group_by(AgeGroup) %>%
  summarise(AvgBloodPressure = mean(BloodPressure))

# Create a bar chart
bar_chart <- ggplot(data = avg_bp_by_age_group, aes(x = AgeGroup, y = AvgBloodPressure)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Average Blood Pressure by Age Group",
       x = "Age Group", y = "Average Blood Pressure")
print(bar_chart)





Day 1 


1.List of Programs:
1The intervals and corresponding frequencies are as follows. age frequency
1-5. 200 
5-15 450 
15-20 300 
20-50 1500 
50-80 700 
80-110 44
Compute an approximate median value for the data


# Create a dataframe with the given data
data <- data.frame(
  age = c("1-5", "5-15", "15-20", "20-50", "50-80", "80-110"),
  frequency = c(200, 450, 300, 1500, 700, 44)
)

# Calculate cumulative frequencies
data$cumulative_frequency <- cumsum(data$frequency)

# Find the interval containing the median
total_frequency <- sum(data$frequency)
median_cf <- total_frequency / 2
median_interval <- data$age[which(data$cumulative_frequency >= median_cf)[1]]

# Calculate the median value within the median interval
median_interval_values <- as.numeric(strsplit(median_interval, "-")[[1]])
median_lower_limit <- median_interval_values[1]
median_upper_limit <- median_interval_values[2]
median_interval_width <- median_upper_limit - median_lower_limit
median_cumulative_frequency_before <- data$cumulative_frequency[which(data$age == median_interval) - 1]
median_frequency_within_interval <- data$frequency[which(data$age == median_interval)]

median <- median_lower_limit + ((median_cf - median_cumulative_frequency_before) / median_frequency_within_interval) * median_interval_width

print(paste("Approximate median value:", median))





2.Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.
(a) What is the mean of the data? What is the median?
(b) What is the mode of the data? Comment on the data’s modality (i.e., bimodal, trimodal, etc.).
(c) What is the midrange of the data?
(d) Can you find (roughly) the first quartile (Q1) and the third quartile (Q3) of the data?


A.
# Age values
ages <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)

# Calculate mean
mean_age <- mean(ages)

# Calculate median
median_age <- median(ages)

print(paste("Mean:", mean_age))
print(paste("Median:", median_age))

B.# Calculate mode
mode_age <- as.numeric(names(sort(table(ages), decreasing = TRUE)[1]))

# Determine modality
num_modes <- length(table(ages))
modality <- ifelse(num_modes == 1, "Unimodal",
                   ifelse(num_modes == 2, "Bimodal",
                          ifelse(num_modes == 3, "Trimodal",
                                 "Multimodal")))

print(paste("Mode:", mode_age))
print(paste("Modality:", modality))


C.
# Calculate midrange
midrange_age <- (max(ages) + min(ages)) / 2

print(paste("Midrange:", midrange_age))

D.# Calculate quartiles
q1 <- quantile(ages, 0.25)
q3 <- quantile(ages, 0.75)

print(paste("First Quartile (Q1):", q1))
print(paste("Third Quartile (Q3):", q3))




   3.Data Preprocessing :Reduction and Transformation
   Use the two methods below to normalize the following group of data: 200, 300, 400, 600, 1000 (a) min-max normalization by setting min = 0 and max = 1 (b) z-score normalization

A)# Given data
data <- c(200, 300, 400, 600, 1000)

# Min-max normalization
min_value <- min(data)
max_value <- max(data)
normalized_data_minmax <- (data - min_value) / (max_value - min_value)

print("Min-Max Normalized Data:")
print(normalized_data_minmax)

B)# Z-score normalization
mean_value <- mean(data)
std_deviation <- sd(data)
normalized_data_zscore <- (data - mean_value) / std_deviation

print("Z-Score Normalized Data:")
print(normalized_data_zscore)



  4.Data:11,13,13,15,15,16,19,20,20,20,21,21,22,23,24,30,40,45,45,45,71,
72,73,75
 
      a) Smoothing by bin mean
b) Smoothing by bin median
c) Smoothing by bin boundaries

A)data <- c(11, 13, 13, 15, 15, 16, 19, 20, 20, 20, 21, 21, 22, 23, 24, 30, 40, 45, 45, 45, 71, 72, 73, 75)
bin_size <- 5

# Create bins
bins <- seq(min(data), max(data), by = bin_size)

# Calculate bin means
bin_means <- tapply(data, cut(data, breaks = bins), mean)

print("Smoothing by Bin Mean:")
print(bin_means)

B.# Calculate bin medians
bin_medians <- tapply(data, cut(data, breaks = bins), median)

print("Smoothing by Bin Median:")
print(bin_medians)

C.# Calculate bin boundaries
bin_boundaries <- seq(min(data), max(data) + bin_size, by = bin_size)

print("Smoothing by Bin Boundaries:")
print(bin_boundaries)


5. Suppose that a hospital tested the age and body fat data for 18 randomly selected adults with the following results: 
 
(a) Calculate the mean, median, and standard deviation of age and %fat.
(b)  Draw the boxplots for age and %fat.
(c) Draw a scatter plot and a q-q plot based on these two variables.



# Age and %fat data (replace with actual data)
age <- c(...)
percent_fat <- c(...)

# Calculate mean
mean_age <- mean(age)
mean_percent_fat <- mean(percent_fat)

# Calculate median
median_age <- median(age)
median_percent_fat <- median(percent_fat)

# Calculate standard deviation
sd_age <- sd(age)
sd_percent_fat <- sd(percent_fat)

print("Mean Age:", mean_age)
print("Median Age:", median_age)
print("Standard Deviation Age:", sd_age)

print("Mean %Fat:", mean_percent_fat)
print("Median %Fat:", median_percent_fat)
print("Standard Deviation %Fat:", sd_percent_fat)
# Create boxplot for age
boxplot(age, main="Boxplot of Age")

# Create boxplot for %fat
boxplot(percent_fat, main="Boxplot of %Fat")
# Create scatter plot
plot(age, percent_fat, main="Scatter Plot", xlab="Age", ylab="%Fat")

# Create Q-Q plot for age
qqnorm(age)
qqline(age, col = 2)

# Create Q-Q plot for %fat
qqnorm(percent_fat)
qqline(percent_fat, col = 2)

6.Suppose that a hospital tested the age and body fat data for 18 randomly selected adults with the following results: 
(i) Use min-max normalization to transform the value 35 for age onto the range [0.0, 1.0].
(ii) Use z-score normalization to transform the value 35 for age, where the standard deviation of age is 12.94 years.
(iii) Use normalization by decimal scaling to transform the value 35 for age. Perform the above functions using R – tool 
# Given value
value <- 35

# Min-Max Normalization
min_value <- 0
max_value <- 1
minmax_normalized <- (value - min_value) / (max_value - min_value)

print(paste("Min-Max Normalized value:", minmax_normalized))

# Z-Score Normalization
mean_age <- 0 # Assume mean of age is 0 for simplicity
std_deviation_age <- 12.94
zscore_normalized <- (value - mean_age) / std_deviation_age

print(paste("Z-Score Normalized value:", zscore_normalized))

# Normalization by Decimal Scaling
power <- floor(log10(max(abs(value)))) + 1
decimal_scaled <- value / (10 ^ power)

print(paste("Normalization by Decimal Scaling:", decimal_scaled))




7.The following values are the number of pencils available in the different boxes. Create a vector and find out the mean, median and mode values of set of pencils in the given data. 
Box1 Box2 Box3 Box4 Box5 Box6 Box7 Box8 Box9 Box 10
9          25      23     12      11      6      7        8        9             10 

# Create a vector with the given data
pencils <- c(9, 25, 23, 12, 11, 6, 7, 8, 9, 10)

# Calculate mean
mean_pencils <- mean(pencils)

# Calculate median
median_pencils <- median(pencils)

# Calculate mode (using a custom function since base R doesn't have a built-in mode function)
get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
mode_pencils <- get_mode(pencils)

print(paste("Mean:", mean_pencils))
print(paste("Median:", median_pencils))
print(paste("Mode:", mode_pencils))



8. the following table would be plotted as (x,y) points, with the first column being the x values as number of mobile phones sold and the second column being the y values as money. To use the scatter plot for how many mobile phones sold. 
x :4 1 5 7 10 2 50 25 90 36 
y :12 5 13 19 31 7 153 72 275 110 
# Given data
x <- c(4, 1, 5, 7, 10, 2, 50, 25, 90, 36)
y <- c(12, 5, 13, 19, 31, 7, 153, 72, 275, 110)

# Create a scatter plot
plot(x, y, main="Scatter Plot of Mobile Phones Sold",
     xlab="Number of Mobile Phones Sold", ylab="Money")

# Add a grid to the plot (optional)
grid()

# Show the plot




9. Implement of the R script using marks scored by a student in his model exam has been sorted as follows: 55, 60, 71, 63, 55, 65, 50, 55,58,59,61,63,65,67,71,72,75. Partition them into three bins by each of the following methods. Plot the data points using histogram. 
(a) equal-frequency (equi-depth) partitioning (b) equal-width partitioning 

# Given data
marks <- c(55, 60, 71, 63, 55, 65, 50, 55, 58, 59, 61, 63, 65, 67, 71, 72, 75)

# Calculate number of bins
num_bins <- 3

# (a) Equal-Frequency (Equi-Depth) Partitioning
bin_breaks_eq_freq <- quantile(marks, probs = seq(0, 1, length.out = num_bins + 1))
marks_binned_eq_freq <- cut(marks, breaks = bin_breaks_eq_freq, include.lowest = TRUE)

# (b) Equal-Width Partitioning
bin_width <- (max(marks) - min(marks)) / num_bins
bin_breaks_eq_width <- seq(min(marks), max(marks) + bin_width, by = bin_width)
marks_binned_eq_width <- cut(marks, breaks = bin_breaks_eq_width, include.lowest = TRUE)

# Plot histograms
par(mfrow = c(1, 2))  # Set up a 1x2 grid for two histograms

hist(marks, breaks = bin_breaks_eq_freq, main = "Equal-Frequency Partitioning",
     xlab = "Marks", ylab = "Frequency", col = "lightblue")
abline(v = bin_breaks_eq_freq, col = "red", lwd = 2)

hist(marks, breaks = bin_breaks_eq_width, main = "Equal-Width Partitioning",
     xlab = "Marks", ylab = "Frequency", col = "lightgreen")
abline(v = bin_breaks_eq_width, col = "red", lwd = 2)

10. Suppose that the speed car is mentioned in different driving style. 
Regular 78.3 81.8 82 74.2 83.4 84.5 82.9 77.5 80.9 70.6 Speed 
Calculate the Inter quantile and standard deviation of the given data. 


# Given data
speed_data <- c(78.3, 81.8, 82, 74.2, 83.4, 84.5, 82.9, 77.5, 80.9, 70.6)

# Calculate Interquartile Range (IQR)
q1 <- quantile(speed_data, 0.25)
q3 <- quantile(speed_data, 0.75)
iqr <- q3 - q1

# Calculate Standard Deviation
sd_speed <- sd(speed_data)

print(paste("Interquartile Range (IQR):", iqr))
print(paste("Standard Deviation:", sd_speed))

	
11.Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.
Can you find (roughly) the first quartile (Q1) and the third quartile (Q3) of the data?
# Given data
age_data <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)

# Calculate the positions for Q1 and Q3
total_data_points <- length(age_data)
q1_position <- (total_data_points + 1) / 4  # 25% of data
q3_position <- 3 * q1_position  # 75% of data

# Find the values at the approximate positions
q1 <- age_data[ceiling(q1_position)]
q3 <- age_data[ceiling(q3_position)]

print(paste("Approximate First Quartile (Q1):", q1))
print(paste("Approximate Third Quartile (Q3):", q3))



1.Covariance and correlation 
Children of three ages are asked to indicate their preference for three photographs of adults. Do the data suggest that there is a significant relationship between age and photograph preference? What is wrong with this study?

                                                                        Photograph:
                         Age of child               A                      B                      C
         				   5-6 years:      18                     22                     20

                                    7-8 years:         2                    28                     40

                                    9-10 years:     20                     10                     40

1.	Use cov() to calculate the sample covariance between B  and  C.
2.	Use another call to cov() to calculate the sample covariance matrix for the preferences.
3.	Use cor() to calculate the sample correlation between B and C.
4.	Use another call to cor() to calculate the sample correlation matrix for the preferences.
# Creating a matrix with the provided preference data
preferences <- matrix(c(22, 28, 10, 20, 40, 40), ncol = 3, byrow = TRUE)
rownames(preferences) <- c("5-6 years", "7-8 years", "9-10 years")
colnames(preferences) <- c("A", "B", "C")
# Calculate the sample covariance between B and C
cov_bc <- cov(preferences[, "B"], preferences[, "C"])
print("Sample Covariance between B and C:")
print(cov_bc)
# Calculate the sample covariance matrix for the preferences
cov_matrix <- cov(preferences)
print("Sample Covariance Matrix for Preferences:")
print(cov_matrix)
# Calculate the sample correlation between B and C
cor_bc <- cor(preferences[, "B"], preferences[, "C"])
print("Sample Correlation between B and C:")
print(cor_bc)

# Calculate the sample correlation matrix for the preferences
cor_matrix <- cor(preferences)
print("Sample Correlation Matrix for Preferences:")
print(cor_matrix)
18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 25, 25, 25, 25, 25, 28, 28, 30, 
30, 30. 
(i) Partition the dataset using an equal-frequency partitioning method with bin equal to 3 (ii) apply data smoothing using bin means and bin boundary.
(iii) Plot Histogram for the above frequency division 
2.Imagine that you have selected data from the All Electronics data warehouse for analysis. The data set will be huge! The following data are a list of All Electronics prices for commonly sold items (rounded to the nearest dollar). The numbers have been sorted: 1, 1, 5, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 14, 15, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18, 






3.Two Maths teachers are comparing how their Year 9 classes performed in the end of year exams. Their results are as follows:
Class A: 76, 35, 47, 64, 95, 66, 89, 36, 8476,35,47,64,95,66,89,36,84 
Class B: 51, 56, 84, 60, 59, 70, 63, 66, 5051,56,84,60,59,70,63,66,50 
(i) Find which class had scored higher mean, median and range.
(ii) Plot above in boxplot and give the inferences 
Class B: 51, 56, 84, 60, 59, 70, 63, 66, 5051,56,84,60,59,70,63,66,50 





# Data for Class A and Class B
classA <- c(76, 35, 47, 64, 95, 66, 89, 36, 84)
classB <- c(51, 56, 84, 60, 59, 70, 63, 66, 50)

# Calculate mean, median, and range for Class A
mean_A <- mean(classA)
median_A <- median(classA)
range_A <- max(classA) - min(classA)

# Calculate mean, median, and range for Class B
mean_B <- mean(classB)
median_B <- median(classB)
range_B <- max(classB) - min(classB)

# Compare means
if (mean_A > mean_B) {
  mean_higher <- "Class A"
} else if (mean_A < mean_B) {
  mean_higher <- "Class B"
} else {
  mean_higher <- "Both classes have the same mean"
}

# Compare medians
if (median_A > median_B) {
  median_higher <- "Class A"
} else if (median_A < median_B) {
  median_higher <- "Class B"
} else {
  median_higher <- "Both classes have the same median"
}

# Compare ranges
if (range_A > range_B) {
  range_higher <- "Class A"
} else if (range_A < range_B) {
  range_higher <- "Class B"
} else {
  range_higher <- "Both classes have the same range"
}

# Print results
cat("Mean: Class", mean_higher, "had a higher mean.\n")
cat("Median: Class", median_higher, "had a higher median.\n")
cat("Range: Class", range_higher, "had a higher range.\n")
# Combine data for both classes
combined_data <- data.frame(Class = c(rep("A", length(classA)), rep("B", length(classB))),
                             Score = c(classA, classB))

# Create a boxplot
boxplot(Score ~ Class, data = combined_data,
        main = "Boxplot of Exam Scores by Class",
        xlab = "Class", ylab = "Scores")

# Provide inferences
cat("\nInferences based on the boxplot:\n")
cat("- Class A has a wider range of scores compared to Class B.\n")
cat("- Class A's distribution appears to be skewed towards higher scores, while Class B's distribution is more symmetric.\n")
cat("- Class A has a higher median and a more spread-out interquartile range compared to Class B.\n")



4.Let us consider one example to make the calculation method clear. Assume that the minimum and maximum values for the feature F are $50,000 and $100,000 correspondingly. It needs to range F from 0 to 1. In accordance with min-max normalization, v = $80,
b) Use the two methods below to normalize the following group of data: 200, 300, 400, 600, 1000
 (a) min-max normalization by setting min = 0 and max = 1


 (b) z-score normalization

data <- c(200, 300, 400, 600, 1000)
v <- 80
min_value <- 50000
max_value <- 100000
# Min-Max Normalization formula: (x - min) / (max - min)
min_max_normalized_a <- (data - min(data)) / (max(data) - min(data))

cat("Min-Max Normalization (Method A):\n")
print(min_max_normalized_a)
# Z-Score Normalization formula: (x - mean) / standard deviation
z_score_normalized <- (data - mean(data)) / sd(data)

cat("\nZ-Score Normalization:\n")
print(z_score_normalized)





5.	Make a histogram for the “AirPassengers “dataset, start at 100 on the x-axis, and from values 200 to 700, make the bins 150 wide


# Load the necessary library
library(datasets)

# Load the AirPassengers dataset
data("AirPassengers")

# Set up the histogram parameters
start_value <- 100
end_value <- 700
bin_width <- 150

# Create the histogram
hist(AirPassengers, breaks = seq(start_value, end_value, by = bin_width),
     xlab = "Passenger Count", ylab = "Frequency",
     main = "Histogram of AirPassengers",
     col = "skyblue", border = "black")


6.	Obtain Multiple Lines in Line Chart using a single Plot Function in R.Use attributes“mpg”and“qsec”of   the dataset “mtcars”

# Load the necessary dataset
data("mtcars")
# Create a line chart with the first line (mpg)
plot(mtcars$qsec, mtcars$mpg, type = "l", col = "blue", xlab = "qsec", ylab = "mpg", main = "Line Chart with Multiple Lines")
# Add the second line (qsec) using the lines() function
lines(mtcars$qsec, mtcars$qsec, col = "red")

7.	Download the Dataset "water" From R dataset Link.Find out whether there is a linear relation between attributes"mortality" and"hardness" by plot function.Fit the Data into the Linear Regression model.Predict the mortality for the hardness=88.

# Load the dataset
water_data <- read.csv("water.csv")

# Display the structure of the dataset
str(water_data)

# Create a scatter plot
plot(water_data$hardness, water_data$mortality,
     xlab = "Hardness", ylab = "Mortality",
     main = "Scatter Plot of Mortality vs. Hardness")

# Fit a linear regression model
linear_model <- lm(mortality ~ hardness, data = water_data)

# Display the model summary
summary(linear_model)
# Predict mortality for hardness = 88
new_data <- data.frame(hardness = 88)
predicted_mortality <- predict(linear_model, newdata = new_data)
cat("Predicted Mortality for Hardness = 88:", predicted_mortality)



8.	Create a Boxplot graph for the relation between "mpg"(miles per galloon) and "cyl"(number of Cylinders) for the dataset "mtcars" available in R Environment.

# Load the necessary dataset
data("mtcars")

# Create a boxplot graph
boxplot(mpg ~ cyl, data = mtcars,
        xlab = "Number of Cylinders", ylab = "Miles per Gallon",
        main = "Boxplot of MPG by Number of Cylinders",
        col = "skyblue", border = "black")






   9. Assume the Tennis coach wants to determine if any of his team players are scoring   
      outliers. To visualize the distribution of points scored by his players, then how can he    
      decide to develop the box plot? Give suitable example using Boxplot visualization   
      technique.


# Example dataset of points scored by tennis players
points_scored <- c(10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 100, 120, 150)

# Create a boxplot to visualize points scored
boxplot(points_scored,
        main = "Boxplot of Points Scored by Tennis Players",
        ylab = "Points Scored")

# Add a title and labels

10. Implement using R language in which age group of people are affected byblood pressure based on the diabetes dataset show it using scatterplot and bar chart (that is BloodPressure vs Age using dataset “diabetes.csv”)

# Load the necessary library
library(ggplot2)

# Load the dataset
diabetes_data <- read.csv("diabetes.csv")

# Display the structure of the dataset
str(diabetes_data)
# Create a scatterplot
scatter_plot <- ggplot(data = diabetes_data, aes(x = Age, y = BloodPressure)) +
  geom_point() +
  labs(title = "Scatterplot of BloodPressure vs Age",
       x = "Age", y = "BloodPressure")
print(scatter_plot)
# Create age groups
age_group_data <- diabetes_data %>%
  mutate(AgeGroup = cut(Age, breaks = c(20, 30, 40, 50, 60, 70, Inf),
                         labels = c("20-29", "30-39", "40-49", "50-59", "60-69", "70+")))

# Calculate average blood pressure by age group
avg_bp_by_age_group <- age_group_data %>%
  group_by(AgeGroup) %>%
  summarise(AvgBloodPressure = mean(BloodPressure))

# Create a bar chart
bar_chart <- ggplot(data = avg_bp_by_age_group, aes(x = AgeGroup, y = AvgBloodPressure)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Average Blood Pressure by Age Group",
       x = "Age Group", y = "Average Blood Pressure")
print(bar_chart)

Consider the data set and perform the Apriori Algorithm and FP algorithm support:3 and confidence=50%
 

Consider the data set and perform the Apriori Algorithm and FP algorithm support:3 and confidence=50%
Consider the market basket transactions shown in the above table.
(a) What is the maximum number of association rules that can be extracted
from this data (including rules that have zero support)?
(b) What is the maximum size of frequent itemsets that can be extracted
(assuming minsup > 0)?
 

Bayes classification and descion tree (using training and test data)
 


3.Analysis the dataset “diabetes. csv” how the diabetes trend is for different age people, using linear regression and multiple regression.

# Load necessary libraries
library(readr)

# Load the dataset
diabetes_data <- read_csv("diabetes.csv")

# Display the structure of the dataset
str(diabetes_data)

# Display summary statistics
summary(diabetes_data)

# Load necessary libraries
library(ggplot2)

# Linear regression model
linear_model <- lm(DiabetesOutcome ~ Age, data = diabetes_data)

# Summary of the linear regression model
summary(linear_model)

# Scatter plot with regression line
ggplot(diabetes_data, aes(x = Age, y = DiabetesOutcome)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Linear Regression: Diabetes Trend by Age")

# Multiple regression model
multiple_model <- lm(DiabetesOutcome ~ Age + BloodPressure + BMI, data = diabetes_data)

# Summary of the multiple regression model
summary(multiple_model)



Implement using WEKA for the given Suppose a database has five transactions. Let min sup= 50%(2) and min con f = 80%. 
	Transactions		Items 
	T1		(M, O, N, K, E, Y)
	T2		(D, O, N, K, E, Y)
	T3 		(M, A, K, E)
	T4		(M, U, C, K, Y)
	T5		(C,O, O, K, I ,E)
•	Find all frequent item sets using Apriori algorithm 
•	Also draw FP-Growth Tree 

				
 Prediction of Categorical Data using Decision Tree Algorithm through WEKA using any datasets. a) Tree b) Preprocess c) Logistic

Create the dataset using ARFF file format: 
a.Find the frequent itemsets and generate association rules on this. Assume that minimum support threshold (s = 33.33%) and minimum confident threshold (c = 60%).
b.List the various rule generated by apriori and FP tree algorthim ,mention wheather accepted or rejcted.


Prediction of Categorical Data using Rule base classification and decision tree classification  through WEKA using any datasets.  Compare the accuracy using two algorithm and plot the graph

4.The following list of persons with vegetarian or not details given in the table. How will      you find out how many of them are vegetarian and how many of them are non-vegetarian? Which type of the person total count is greater value?

Person	Gopu	Babu	Baby	Gopal	Krishna	Jai	Dev	Malini	Hema	Anu
Vegetarian	yes	yes	yes	no	yes	no	no	yes	yes	yes

num_vegetarian <- sum(vegetarian_status == "yes")
num_non_vegetarian <- sum(vegetarian_status == "no")

print(paste("Number of Vegetarian Individuals:", num_vegetarian))
print(paste("Number of Non-Vegetarian Individuals:", num_non_vegetarian))
if (num_vegetarian > num_non_vegetarian) {
  print("Vegetarian individuals have a greater count.")
} else if (num_non_vegetarian > num_vegetarian) {
  print("Non-vegetarian individuals have a greater count.")
} else {
  print("The counts of vegetarian and non-vegetarian individuals are equal.")
}
5.The following table would be plotted as (x,y) points, with the first column being the x values as number of mobile phones sold and the second column being the y values as money. To use the scatter plot for how many mobile phones sold.
x	4	1	5	7	10	2	50	25	90	36
y	12	5	13	19	31	7	153	72	275	110




x_values <- c(4, 1, 5, 7, 10, 2, 50, 25, 90, 36)
y_values <- c(12, 5, 13, 19, 31, 7, 153, 72, 275, 110)
install.packages("ggplot2")  # Install ggplot2 package
library(ggplot2)  # Load ggplot2 package
# Create a data frame
data <- data.frame(x = x_values, y = y_values)
# Create a scatter plot
scatter_plot <- ggplot(data, aes(x = x, y = y)) +
  geom_point() +  # Add points to the plot
  labs(title = "Scatter Plot of Mobile Phones Sold", x = "Number of Mobile Phones Sold", y = "Money")  # Set title and axis labels
# Display the scatter plot
print(scatter_plot)






6.Generate rules using FP growth algorithm using the given dataset which has the following transactions with items purchased: Consider the values as support=50% and confidence=75%.
 


7.Prediction of Diabetes Data using Decision tree classifier  in WEKA.  Compare it with Support Vector Machine classifier. Show the result accuracy and F1 measure calculation .Plot the graph and explain the summary of results.


8.Implement of the R script using marks scored by a student in his model exam has been sorted as follows: 55, 60, 71, 63, 55, 65, 50, 55,58,59,61,63,65,67,71,72,75. Partition them into three bins by each of the following methods. Plot the data points using histogram.
(a) equal-frequency (equi-depth) partitioning
(b) equal-width partitioning
 (c) clustering



# Sample data
marks <- c(55, 60, 71, 63, 55, 65, 50, 55, 58, 59, 61, 63, 65, 67, 71, 72, 75)

# Equal-frequency (equi-depth) partitioning
bins_eq_freq <- cut(marks, breaks = 3, labels = c("Low", "Medium", "High"),  
 include.lowest = TRUE)

# Equal-width partitioning
bins_eq_width <- cut(marks, breaks = c(50, 60, 70, 80), labels = c("Low", "Medium", 
 "High"), include.lowest = TRUE)

# Clustering
library(cluster)
kmeans_result <- kmeans(matrix(marks), centers = 3)
bins_clustering <- cutree(kmeans_result, k = 3)

# Plot histograms
par(mfrow = c(1, 3))
hist(marks, main = "Equal-Frequency Partitioning", col = "blue", breaks = 3)
hist(marks, main = "Equal-Width Partitioning", col = "green", breaks = c(50, 60, 70, 
80))
hist(marks, main = "Clustering", col = "purple", breaks = 3)


11,.The given are the strike-rates scored by a batsman in season 1 in different tournaments.  100, 70, 60, 90, 90 
(a)	min-max normalization by setting min = 0 and max = 1 
(b)	z-score normalization 
(c)	z-score normalization using the mean absolute deviation instead of standard deviation
(d)	normalization by decimal scaling


   install.packages("dplyr")  # For data manipulation
library(dplyr)
strike_rates <- c(100, 70, 60, 90, 90)
min_max_normalized <- (strike_rates - min(strike_rates)) / (max(strike_rates) - min(strike_rates))
min_max_normalized
z_score_normalized <- (strike_rates - mean(strike_rates)) / sd(strike_rates)
z_score_normalized
mad <- mean(abs(strike_rates - mean(strike_rates)))
mad_normalized <- (strike_rates - mean(strike_rates)) / mad
mad_normalized
k <- 2
decimal normalized <- strike_rates / 10^k
decimal normalized
print("Min-Max Normalization:")
print(min_max_normalized)
print("Z-Score Normalization:")
print(z_score_normalized)
print("Z-Score Normalization with MAD:")
print(mad normalized)
print("Normalization by Decimal Scaling:")
print(decimal normalized)




 12.Suppose some car is tested for the AvgSpeed and TotalTime data for 9 randomly selected car with the following result
AvgSpeed
(in kph)	78	81	82	74	83	82	77	80	70
TotalTime
(in mins)	39	37	36	42	35	36	40	38	46
a)	Calculate the standard deviation of AvgSpeed and TotalTime.
b)	Calculate the Variance of  AvgSpeed and TotalTime for the above dataset. 


13.Consider this table 
c)	TID  	items bought
d)	T100 	{M, O, N, K, E, Y}
e)	T200 	{D, O, N, K, E, Y }
f)	T300 	{M, A, K, E}
g)	T400 	{M, U, C, K, Y}
h)	T500 	{C, O, O, K, I ,E}
i)	(a) Find all frequent item set using Apriori and FP-growth, respectively. Compare the efficiency of the two mining processes.
j)	(b) List all of the strong association rules (with support s and confidence c) matching the following metarule, where X is a variable representing customers, and itemi denotes variables representing items (e.g., “A”, “B”, etc.):
k)	∀x ∈ transaction, buys(X, item1) ∧ buys(X, item2) ⇒ buys(X, item3)

install.packages("arules")  # Install the arules package
library(arules)  # Load the arules package
data <- data.frame(
  TID = c("T100", "T200", "T300", "T400", "T500"),
  items = list(
    c("M", "O", "N", "K", "E", "Y"),
    c("D", "O", "N", "K", "E", "Y"),
    c("M", "A", "K", "E"),
    c("M", "U", "C", "K", "Y"),
    c("C", "O", "O", "K", "I", "E")
  ))
# Convert the items column to a transaction format
transactions <- as(data$items, "transactions")

# Perform Apriori
apriori_rules <- apriori(transactions, parameter = list(support = 0.2, confidence = 0.6))

# Perform FP-Growth
fp_growth_rules <- fpgrowth(transactions, parameter = list(support = 0.2, confidence = 0.6))
# Frequent itemsets from Apriori
frequent_itemsets_apriori <- inspect(apriori_rules)

# Frequent itemsets from FP-Growth
frequent_itemsets_fpgrowth <- inspect(fp_growth_rules)

# Print the results
print("Frequent Itemsets from Apriori:")
print(frequent_itemsets_apriori)

print("Frequent Itemsets from FP-Growth:")
print(frequent_itemsets_fpgrowth)

# Specify the metarule
metarule <- "buys(X, item1) & buys(X, item2) => buys(X, item3)"

# Generate rules using the metarule
association_rules <- apriori(transactions, parameter = list(support = 0.2, confidence = 0.6), appearance = list(lhs = metarule))

# Print the generated rules
print("Association Rules Matching the Metarule:")
print(association_rules)
